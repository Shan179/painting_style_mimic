{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from six.moves import urllib\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import vgg_model #to use the pretrained vgg_model\n",
    "import utils #some utility functions, like resizing the image, saving the image etc to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#name of the style image\n",
    "STYLE = 'starry_night'\n",
    "#name of the content image\n",
    "CONTENT = 'tubingen'\n",
    "STYLE_IMAGE = 'styles/' + STYLE + '.jpg'\n",
    "CONTENT_IMAGE = 'content/' + CONTENT + '.jpg'\n",
    "IMAGE_HEIGHT = 250\n",
    "IMAGE_WIDTH = 333\n",
    "NOISE_RATIO = 0.6 # percentage of weight of the noise for intermixing with the content image\n",
    "\n",
    "# Layers used for style features. You can change this.\n",
    "STYLE_LAYERS = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
    "W = [0.5, 1.0, 1.5, 3.0, 4.0] # give more weights to deeper layers.\n",
    "\n",
    "# Layer used for content features. You can change this.\n",
    "CONTENT_LAYER = 'conv4_2'\n",
    "\n",
    "ITERS = 300\n",
    "#learning rate\n",
    "LR = 2.0\n",
    "\n",
    "#mean of the pixels of the images that were used to train VGG net \n",
    "MEAN_PIXELS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "\n",
    "VGG_DOWNLOAD_LINK = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n",
    "VGG_MODEL = 'imagenet-vgg-verydeep-19.mat'\n",
    "EXPECTED_BYTES = 534904783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the content loss function\n",
    "def _create_content_loss(p, f):\n",
    "    #inputs \n",
    "        #p: feature map of the content image\n",
    "        #f: feature map of the input_image\n",
    "    #outputs the content loss which is defined as: summation((f-p)^2)/(4*size_of_feature_map)\n",
    "    content_loss = tf.reduce_sum(tf.square(f-p))/(4*p.size)\n",
    "    return content_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the gram matrix function\n",
    "def _gram_matrix(F, N, M):\n",
    "    #inputs\n",
    "        #F: feature maps \n",
    "        #M: width*height of a feature map\n",
    "        #N: depth of the feature map\n",
    "    #outputs the gram matrix which gives the correlation between the pair of activations and is defined as F'*F\n",
    "    F = tf.reshape(F,(M,N))\n",
    "    G = tf.matmul(tf.transpose(F),F)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the style loss function for a particular layer\n",
    "def _single_style_loss(a, g):\n",
    "    #inputs\n",
    "        #a: feature map of the style image\n",
    "        #g: feature map of the input_image\n",
    "    #outputs the style loss for a particular activation layer,\n",
    "    #defined as sum of squares of the differences between the gram matrices of both a and g\n",
    "    M = a.shape[1]*a.shape[2]\n",
    "    N = a.shape[3]\n",
    "    #getting the gram matrices\n",
    "    G_a = _gram_matrix(a,N,M)\n",
    "    G_g = _gram_matrix(g,N,M)\n",
    "    style_loss = (0.5/(M*N)**2)*tf.reduce_sum(tf.square(G_a-G_g))\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the function which calculates the total style loss\n",
    "def _create_style_loss(A, model):\n",
    "    #inputs\n",
    "        #A: A is a dictionary containing the feature maps for different conv layers\n",
    "        #model: Our computaional graph containing activation of the input image in different layers, along with other\n",
    "                #parameters required for optimization\n",
    "    n_layers = len(STYLE_LAYERS)\n",
    "    style_losses = []\n",
    "    for i in range(n_layers):\n",
    "        style_losses.append(_single_style_loss(A[STYLE_LAYERS[i]],model[STYLE_LAYERS[i]]))\n",
    "    loss = 0\n",
    "    for i in range(n_layers):\n",
    "        loss += W[i]*style_losses[i] #remember W is a list of weights of contribution of different layers to the style loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Defining the function create_losses which finds the net loss, content loss + style loss\n",
    "def _create_losses(model,content_activations,style_activations):\n",
    "    #inputs\n",
    "        #model: Our computaional graph containing activation of the \n",
    "                #input image in different layers, along with other\n",
    "                #parameters required for optimization\n",
    "        #content_image:a dictionary which contains the activations \n",
    "                        #of the style_image at\n",
    "                        #different layers of the convnet\n",
    "        #style_activations: a dictionary which contains the activations \n",
    "                            #of the style_image at\n",
    "                            #different layers of the convnet\n",
    "        \n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        \n",
    "        #finding content loss\n",
    "        content_loss = _create_content_loss(content_activations[CONTENT_LAYER], model[CONTENT_LAYER])\n",
    "        \n",
    "        #finding style loss\n",
    "        style_loss = _create_style_loss(style_activations, model)\n",
    "\n",
    "        total_loss = 0.001*content_loss + 1*style_loss #total loss as the weigthed sum of content and style loss\n",
    "                                                      #feel free to tweak the weights, 1/50 and 1/20 are also quite common  \n",
    "\n",
    "    return content_loss, style_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training our model\n",
    "def train(model, generated_image, initial_image):\n",
    "    #inputs\n",
    "        #model: Our computaional graph containing activation of the input image in different layers, along with other\n",
    "                #parameters required for optimization\n",
    "        #generated_image: imput image\n",
    "        #initial_image: the initial image, which is just the content image with some noise\n",
    "    skip_step = 1\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        #initializing the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #here we are assigning the initial_image to the variable generated_image which is nothing but our input image\n",
    "        sess.run(generated_image.assign(initial_image))\n",
    "        #checking whether a checkpoint exists. A checkpoint is basically used to save our model while training, \n",
    "        #so that we dont need to start the training from scratch every time.\n",
    "        #in our case since we are running this for the first time there wont be any checkpoint initially\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        #global_step basically contains the number of training steps which have happened till now i.e number of\n",
    "        #optimization steps\n",
    "        initial_step = model['global_step'].eval()\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #Training start\n",
    "        for index in range(initial_step, ITERS):\n",
    "            if index >= 5 and index < 20:\n",
    "                skip_step = 10\n",
    "            elif index >= 20:\n",
    "                skip_step = 20\n",
    "            #skip_step is used to print the losses, save the checkpoint and generated images at specific steps of iteration\n",
    "            #and not at all the iterations. you can choose it as whatever you like.\n",
    "            #running the optimizer\n",
    "            sess.run(model['optimizer'])\n",
    "            if (index + 1) % skip_step == 0:\n",
    "                #getting the generated image and total loss at the skip_step\n",
    "                gen_image = sess.run(generated_image)\n",
    "                total_loss = sess.run(model['total_loss'])\n",
    "                gen_image = gen_image + MEAN_PIXELS\n",
    "                #printing the total loss\n",
    "                print('Step {}\\n   Sum: {:5.1f}'.format(index + 1, np.sum(gen_image)))\n",
    "                print('   Loss: {:5.1f}'.format(total_loss))\n",
    "                print('   Time: {}'.format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                #saving the image\n",
    "                filename = 'outputs/%d.png' % (index)\n",
    "                utils.save_image(filename, gen_image)\n",
    "                #saving the checkpoint\n",
    "                if (index + 1) % 20 == 0:\n",
    "                    saver.save(sess, 'checkpoints/style_transfer', index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/style_transfer-19\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('input') as scope:\n",
    "    # use variable instead of placeholder because we're training the intial image to make it\n",
    "    # look like both the content image and the style image\n",
    "    input_image = tf.Variable(np.zeros([1, IMAGE_HEIGHT, IMAGE_WIDTH, 3]), dtype=tf.float32)\n",
    "\n",
    "#Downloading the pre trained VGG_MODEL weights    \n",
    "utils.download(VGG_DOWNLOAD_LINK, VGG_MODEL, EXPECTED_BYTES)\n",
    "#model is our computational graph, conataing activations of the input image in different layers and other things\n",
    "#like optimizer,loss essential for training\n",
    "model = vgg_model.load_vgg(VGG_MODEL, input_image)\n",
    "\n",
    "\n",
    "#resizing and normalizing the content image\n",
    "content_image = utils.get_resized_image(CONTENT_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "content_image = content_image - MEAN_PIXELS\n",
    "style_image = utils.get_resized_image(STYLE_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "style_image = style_image - MEAN_PIXELS\n",
    "\n",
    "#getting the dictionary content_activations which\n",
    "#contains the activation of the content_image at\n",
    "#different layers of the convnet\n",
    "with tf.Session() as sess:\n",
    "    sess.run(input_image.assign(content_image))\n",
    "    content_activations = sess.run(model)\n",
    "\n",
    "#getting the dictionary style_activations which\n",
    "#contains the activation of the style_image at\n",
    "#different layers of the convnet\n",
    "with tf.Session() as sess:\n",
    "    sess.run(input_image.assign(style_image))\n",
    "    style_activations = sess.run(model)\n",
    "\n",
    "#getting the content_loss, style_loss and total_loss\n",
    "model['content_loss'], model['style_loss'], model['total_loss'] = _create_losses(model,\n",
    "                                                    content_activations,style_activations)\n",
    "model['global_step'] = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "#creating an optimizer, I am using Adam Optimizer, you can also use RMS Prop or Adagrad\n",
    "model['optimizer'] = tf.train.AdamOptimizer(LR).minimize(model['total_loss'],model['global_step'])\n",
    "\n",
    "#initializing the initial image as the content_image with some noise generated over it\n",
    "initial_image = utils.generate_noise_image(content_image, IMAGE_HEIGHT, IMAGE_WIDTH, NOISE_RATIO)\n",
    "#Let the training begin\n",
    "train(model, input_image, initial_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
